{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/libra0901/loss-analysis-cka/blob/main/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9Je76l1IIyr"
      },
      "source": [
        "## Import the required libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udnSGSBVDKBz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as  optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd9CL1YmLhZr"
      },
      "source": [
        "## Changing the device to execute on - either CPU or CUDA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBnl3NZLIG-w",
        "outputId": "a8d1ea69-082f-40ab-a173-a3feaade6cc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grqg30JXZa1E"
      },
      "source": [
        "### Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec1bls3ZZalS"
      },
      "outputs": [],
      "source": [
        "class LossFunctions:\n",
        "    def __init__(self, num_classes):\n",
        "        super(LossFunctions, self).__init__()\n",
        "        self.num_classes = int(num_classes)\n",
        "    \n",
        "    # Log (cross-entropy) Loss\n",
        "    def cross_entropy_loss(self, x, y):\n",
        "        loss = F.cross_entropy(x, y)\n",
        "        return loss\n",
        "\n",
        "    # Sum of sqaures\n",
        "    def sos_loss(self, x, y):\n",
        "        ones = torch.sparse.torch.eye(self.num_classes).to(device)\n",
        "        y = ones.index_select(0, y)\n",
        "        m = nn.Softmax(dim=1)\n",
        "        criterion = nn.MSELoss(reduction='sum')\n",
        "        loss = criterion(m(x), y)\n",
        "        return loss\n",
        "    \n",
        "    # Mean Sqaured loss - L2 loss \n",
        "    def mse_loss(self, x, y):\n",
        "        ones = torch.sparse.torch.eye(self.num_classes).to(device)\n",
        "        y = ones.index_select(0, y)\n",
        "        m = nn.Softmax(dim=1)\n",
        "        criterion = nn.MSELoss(reduction='mean')\n",
        "        loss = criterion(m(x), y)\n",
        "        return loss\n",
        "    \n",
        "    # Negative log likelihood = logarithmic softmax\n",
        "    def neg_loglike_loss(self, x, y):\n",
        "        m = nn.LogSoftmax(dim=1)\n",
        "        nll_loss = nn.NLLLoss()\n",
        "        loss = nll_loss(m(x), y)\n",
        "        return loss\n",
        "    \n",
        "    # Expectation Loss - L1 loss - Mean absoulte error\n",
        "    def expectation_loss(self, x, y):\n",
        "        ones = torch.sparse.torch.eye(self.num_classes).to(device)\n",
        "        y = ones.index_select(0, y)\n",
        "        m = nn.Softmax(dim=1)\n",
        "        loss = F.l1_loss(m(x), y)\n",
        "        return loss\n",
        "    \n",
        "    def bce_loss(self, x, y):\n",
        "        ones = torch.sparse.torch.eye(self.num_classes).to(device)\n",
        "        y = ones.index_select(0, y)\n",
        "        loss = F.binary_cross_entropy_with_logits(x, y)\n",
        "        return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk2BX2b7QkNS"
      },
      "source": [
        "## Assign Batch Size and required transformations for your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRJL4LvyIHC_"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "\n",
        "traindata_transforms = transforms.Compose([\n",
        "                        transforms.Resize((32,32)),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "testdata_transforms = transforms.Compose([\n",
        "                        transforms.Resize((32,32)),  \n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgoZrwy8Quml"
      },
      "source": [
        "## Dataloading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVbof9J7Qd_N",
        "outputId": "70760249-f70b-4d82-99d8-b4e137281bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "traindata = datasets.CIFAR10(root=\"\\PATH\", train=True, transform=traindata_transforms, download=True) \n",
        "testdata = datasets.CIFAR10(root=\"\\PATH\", train=False, transform=traindata_transforms, download=True)\n",
        "\n",
        "train_dataloader = DataLoader(traindata, batch_size, shuffle=True) \n",
        "test_dataloader = DataLoader(testdata, batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghGzRt-yYbmF"
      },
      "source": [
        "## Train Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHhCfuLIIHPZ"
      },
      "outputs": [],
      "source": [
        "class Train:\n",
        "    def __init__(self,\n",
        "                 optimizer,\n",
        "                 loss,\n",
        "                 epochs:int, \n",
        "                 modelname:str,\n",
        "                 dataset:str):\n",
        "        \n",
        "        super(Train, self).__init__()\n",
        "        self.optimizer = optimizer\n",
        "        self.optimizer_name = str(optimizer)\n",
        "        self.loss = loss\n",
        "        self.epochs = epochs\n",
        "        self.modelname = modelname\n",
        "    \n",
        "    def model_(self, modelname:str, num_classes:int, dataset:str):\n",
        "        if modelname == 'resnet50':\n",
        "            model = models.resnet50(pretrained=True).to(device)\n",
        "            model.fc = nn.Sequential(\n",
        "                        nn.Linear(2048, 1024, bias=True),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Linear(1024, 512, bias=True),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Linear(512, num_classes, bias=True)\n",
        "                        ).to(device)\n",
        "            return model\n",
        "        elif modelname =='resnet18' and dataset=='\\mnist':\n",
        "            model = models.resnet18(pretrained=False).to(device)\n",
        "            model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)\n",
        "            model.fc = nn.Linear(512, num_classes, bias=True).to(device)\n",
        "            return model\n",
        "        elif modelname == 'resnet18':\n",
        "            model = models.resnet18(pretrained=False).to(device)\n",
        "            model.fc = nn.Linear(512, num_classes, bias=True).to(device)\n",
        "            return model\n",
        "        else:\n",
        "            print('ERROR_model_or_dataset_is_not_found')\n",
        "      \n",
        "    def top1_accuracy(self, outputs, labels):\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        acc = torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "        return acc\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def test(self, model, test_dl, dataset):\n",
        "        model.eval()\n",
        "        for batch in test_dl:\n",
        "            inputs, labels = batch\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = self.loss(outputs, labels)\n",
        "            acc = self.top1_accuracy(outputs, labels)\n",
        "        return loss, acc\n",
        "    \n",
        "    # Training Loop \n",
        "    def train(self, train_dl, test_dl, num_classes, filename, dataset):\n",
        "        history = []\n",
        "        since = time.time()\n",
        "        model = self.model_(modelname=self.modelname,\n",
        "                            num_classes = int(num_classes),\n",
        "                            dataset = dataset)\n",
        "        if self.optimizer_name == 'SGD':\n",
        "            optimizer = self.optimizer(model.parameters(), lr=0.0001, momentum=0.93)\n",
        "        else:\n",
        "            optimizer = self.optimizer(model.parameters(), lr=0.001)\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            model.train()\n",
        "            train_loss = []\n",
        "            train_acc = []\n",
        "            test_loss = []\n",
        "            test_acc = []\n",
        "            result = {}\n",
        "            with tqdm(train_dl, unit=\"batch\") as loop:\n",
        "              for batch in loop:\n",
        "                  inputs, labels = batch\n",
        "                  inputs = inputs.to(device)\n",
        "                  labels = labels.to(device)\n",
        "                  outputs = model(inputs)\n",
        "                  loss = self.loss(outputs, labels)\n",
        "                  acc = self.top1_accuracy(outputs, labels)\n",
        "                  train_acc.append(acc.cpu().detach().numpy())\n",
        "                  train_loss.append(loss.cpu().detach().numpy())\n",
        "                  loop.set_description(f\"Epoch [{epoch}/{self.epochs}]\")\n",
        "                  loop.set_postfix(train_loss=np.average(train_loss),train_acc=np.average(train_acc))\n",
        "                  loss.backward()\n",
        "                  optimizer.step()\n",
        "                  optimizer.zero_grad()\n",
        "              \n",
        "              test_losses,test_accu = self.test(model, test_dl, dataset)\n",
        "              test_loss.append(test_losses.cpu().detach().numpy())\n",
        "              test_acc.append(test_accu.cpu().detach().numpy())       \n",
        "              result['train_loss'] = np.average(train_loss)\n",
        "              result['train_acc'] = np.average(train_acc)\n",
        "              result['test_loss'] = np.average(test_loss)\n",
        "              result['test_acc'] = np.average(test_acc)\n",
        "              print('\\nEpoch',epoch,result)\n",
        "              history.append(result)\n",
        "            \n",
        "        time_elapsed = time.time() - since\n",
        "        print('Training Completed in {:.0f} min {:.0f} sec'.format(time_elapsed//60, time_elapsed%60))\n",
        "        return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6KazuyyYkLS"
      },
      "source": [
        "## Assign:<br/>\n",
        "-----------------------------------------------------------------------<br/>\n",
        "Number of classes (w.r.t dataset) = n : int <br/>\n",
        "-----------------------------------------------------------------------<br/>\n",
        "Loss Function = (from set of declared 6 loss fucntions, assign one)<br/>\n",
        "1.   L<sub>1</sub> = <br/>\n",
        "2.   L<sub>2</sub> = <br/>\n",
        "3.   Softmax Cross Entropy = <br/>\n",
        "4.   Binary Cross Entropy = <br/>\n",
        "5.   Mean-Squared-Error = <br/>\n",
        "6.   Sum-of-Sqaures = <br/>\n",
        "-----------------------------------------------------------------------<br/>\n",
        "## Execute the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "iM3S_K35IHVS",
        "outputId": "02e78443-043c-4757-c9a2-16ff48a79760"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-25c74a415e91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLossFunctions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LossFunctions' is not defined"
          ]
        }
      ],
      "source": [
        "num_classes = 10\n",
        "lf = LossFunctions(num_classes)\n",
        "loss = lf.cross_entropy_loss\n",
        "batch_size = 512\n",
        "optimizer = optim.Adam\n",
        "epochs = 15\n",
        "modelname = 'resnet18'\n",
        "dataset = '\\cifar10'\n",
        "\n",
        "# Declaring Loss Function\n",
        "l = LossFunctions(num_classes)\n",
        "# Training \n",
        "t = Train(optimizer = optimizer,\n",
        "          loss = loss,\n",
        "          epochs = epochs,\n",
        "          modelname = modelname,\n",
        "          dataset = dataset,\n",
        "          )\n",
        "filename = dataset[1:] + modelname\n",
        "\n",
        "history = t.train(\n",
        "            train_dl = train_dataloader,\n",
        "            test_dl = test_dataloader,\n",
        "            num_classes = num_classes,\n",
        "            filename = filename,\n",
        "            dataset = dataset\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5SEKIQoZtyA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_H65_KrZuLk"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HUPU00zZJcU"
      },
      "source": [
        "## Visualize the Model's **CKA**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RKKUUskIHXs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEolIefSIHZ7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbe4AwDOIHcR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Mk2BX2b7QkNS"
      ],
      "name": "tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLnXv3EY54VQwHhzvxW5Al",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}